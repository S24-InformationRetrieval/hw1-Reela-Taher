{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Imports\n",
    "from elasticsearch import Elasticsearch \n",
    "from parser import parse_docs\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads stop words within provided file\n",
    "stopword_path = '/Users/reelataher/hw1-Reela-Taher/IR_data/AP_DATA/stoplist.txt'  \n",
    "\n",
    "with open(stopword_path) as f:\n",
    "  stop_words = set(f.read().split())\n",
    "  \n",
    "# Set stopwords to list\n",
    "stop_words = list(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(['http://localhost:9200/'])\n",
    "\n",
    "index_name = \"ap89\"\n",
    "\n",
    "# Index settings\n",
    "ap89_index = {\n",
    "  \"settings\": { # Reduces resource usage\n",
    "    \"analysis\": {\n",
    "      \"filter\": {\n",
    "        \"english_stop\": {\n",
    "          \"type\": \"stop\",\n",
    "          \"stopwords\": stop_words # Custom stop filter using predefined stopwords \n",
    "        }\n",
    "      },\n",
    "      \"analyzer\": { \n",
    "        \"stopped\": {\n",
    "           \"type\": \"custom\",\n",
    "           \"tokenizer\": \"standard\",\n",
    "           \"filter\": [\n",
    "             \"lowercase\",\n",
    "             \"english_stop\"\n",
    "           ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": { # Field data enabled for sorting and aggregations\n",
    "    \"properties\": {\n",
    "      \"text\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"stopped\", \n",
    "        \"index_options\": \"positions\"  \n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse documents \n",
    "docs = parse_docs() \n",
    "\n",
    "# Create index \n",
    "es.indices.create(index=index_name, body=ap89_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the term vectors dictionary\n",
    "term_vectors = {} \n",
    "\n",
    "# Index documents\n",
    "for doc in docs:\n",
    "  \n",
    "  # Make all text lowercase\n",
    "  text = doc['text'].lower()\n",
    "\n",
    "  # Remove punctuation within text\n",
    "  text = text.translate(str.maketrans('', '', string.punctuation))   \n",
    "\n",
    "  # Filter out stop words\n",
    "  filtered_words = [word for word in text.split() if word not in stop_words]\n",
    "\n",
    "  # Join back to string\n",
    "  doc['text'] = ' '.join(filtered_words)\n",
    "\n",
    "  # Store term vectors from each document into dictionary\n",
    "  term_vectors[doc['DOCNO']] = es.mtermvectors(index=index_name, id=doc['DOCNO'], fields=['text'])\n",
    "\n",
    "  es.index(index=index_name, body=doc, id=doc['DOCNO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary to store term frequencies for each term in each document\n",
    "term_frequency = {}\n",
    "\n",
    "# Calculate term frequencies for each term in each document\n",
    "for id, doc in term_vectors.items():\n",
    "   \n",
    "    # Calculate term frequencies for current document\n",
    "    term_frequency[doc['DOCNO']] = {}\n",
    "    \n",
    "    # Iterate over the terms in the term vector of the current document \n",
    "    for term, info in doc['text']['terms'].items():\n",
    "        term_frequency[doc['DOCNO']][term] = info['term_freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all document lengths\n",
    "total_dl = sum(sum(tf.values()) for tf in term_frequency.values())\n",
    "\n",
    "# Calculate average document length \n",
    "avg_dl = total_dl / len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Okapi TF score for a term in a document\n",
    "def okapi_tf_score(tf, length_doc, avg_corpus_length):\n",
    "    score = tf / (tf + 0.5 + 1.5 * (length_doc / avg_corpus_length))\n",
    "    return score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
