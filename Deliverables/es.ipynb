{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ES built-in to Read and Run All Queries\n",
    "\n",
    "# Required Imports\n",
    "import os\n",
    "import math\n",
    "import string\n",
    "from elasticsearch import Elasticsearch \n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "es = Elasticsearch(['http://localhost:9200/'])\n",
    "\n",
    "index_name = \"ap89\"\n",
    "\n",
    "# Library to remove the suffixes from an English word and obtain its stem\n",
    "parser = PorterStemmer()\n",
    "\n",
    "# Path to the directory containing the files\n",
    "directory_path = '/Users/reelataher/hw1-Reela-Taher/IR_data/AP_DATA/ap89_collection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads stop words within provided file\n",
    "stopword_path = '/Users/reelataher/hw1-Reela-Taher/IR_data/AP_DATA/stoplist.txt'  \n",
    "\n",
    "with open(stopword_path) as f:\n",
    "  stop_words = set(f.read().split())\n",
    "  \n",
    "# Set stopwords to list\n",
    "stop_words = list(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse all documents within directory\n",
    "def parse_docs():\n",
    "\n",
    "    # Initialize a list to store all extracted documents\n",
    "    documents = []\n",
    "\n",
    "    # Store all files within a variable using the os module\n",
    "    for filename in os.listdir(directory_path):\n",
    "\n",
    "        # Construct the full path of the file\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "        # Open each file in the ap89_collection directory for reading\n",
    "        with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "\n",
    "            # Initialize what one document should contain, document ID, and body\n",
    "            single_doc = {'DOCNO': '', 'text': ''}\n",
    "            texts = []\n",
    "            inside_text = False\n",
    "            current_text = ''\n",
    "\n",
    "            # Parsing every individual file\n",
    "            for line in file:\n",
    "\n",
    "                # Checks if this is the beginning of a document and initializes an empty dictionary\n",
    "                if line.startswith('<DOC>'):\n",
    "                    single_doc = {'DOCNO': '', 'text': ''}\n",
    "                    texts = []\n",
    "\n",
    "                # Reads that this is the document ID and stores this in a variable\n",
    "                elif line.startswith('<DOCNO>'):\n",
    "                    single_doc['DOCNO'] = line.split('<DOCNO>')[1].split('</DOCNO>')[0].strip()\n",
    "\n",
    "                # Reads that this is the body of the document and starts appending found text\n",
    "                elif line.startswith('<TEXT>'):\n",
    "                    inside_text = True\n",
    "                    current_text = ''\n",
    "\n",
    "                # Reads that this is the end of the document and stores the final content\n",
    "                elif line.startswith('</TEXT>'):\n",
    "                    inside_text = False\n",
    "                    texts.append(current_text.strip())\n",
    "\n",
    "                elif inside_text:\n",
    "                    current_text += line + ' '\n",
    "\n",
    "                # Reads that this is the end of the document\n",
    "                elif line.startswith('</DOC>'):\n",
    "                    single_doc['text'] = ' '.join(texts)\n",
    "                    documents.append(single_doc)\n",
    "\n",
    "    # Returns a list of all parsed documents\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index settings\n",
    "ap89_index = {\n",
    "  \"settings\": { # Reduces resource usage\n",
    "    \"analysis\": {\n",
    "      \"filter\": {\n",
    "        \"english_stop\": {\n",
    "          \"type\": \"stop\",\n",
    "          \"stopwords\": stop_words # Custom stop filter using predefined stopwords \n",
    "        }\n",
    "      },\n",
    "      \"analyzer\": { \n",
    "        \"stopped\": {\n",
    "           \"type\": \"custom\",\n",
    "           \"tokenizer\": \"standard\",\n",
    "           \"filter\": [\n",
    "             \"lowercase\",\n",
    "             \"english_stop\"\n",
    "           ]\n",
    "        } \n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": { # Field data enabled for sorting and aggregations\n",
    "    \"properties\": {\n",
    "      \"text\": {\n",
    "        \"type\": \"text\",\n",
    "        \"term_vector\": \"yes\",\n",
    "        \"analyzer\": \"stopped\", \n",
    "        \"index_options\": \"positions\"  \n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse documents \n",
    "docs = parse_docs() \n",
    "\n",
    "# # Create index \n",
    "es.indices.create(index=index_name, body=ap89_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index documents\n",
    "for doc in docs:\n",
    "  \n",
    "  # Make all text lowercase\n",
    "  text = doc['text'].lower()\n",
    "\n",
    "  # Remove punctuation within text\n",
    "  text = text.translate(str.maketrans('', '', string.punctuation))   \n",
    "\n",
    "  # Filter out stop words\n",
    "  filtered_words = [word for word in text.split() if word not in stop_words]\n",
    "\n",
    "  # Stem each word in the filtered_words list\n",
    "  stemmed_words = [parser.stem(word) for word in filtered_words]\n",
    "\n",
    "  # Join back to string\n",
    "  doc['text'] = ' '.join(stemmed_words)\n",
    "\n",
    "  es.index(index=index_name, body=doc, id=doc['DOCNO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing the query file\n",
    "query_path = '/Users/reelataher/hw1-Reela-Taher/IR_data/AP_DATA/query_desc.51-100.short.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess queries\n",
    "def process_query(query):\n",
    "\n",
    "    # Lowercase\n",
    "    query = query.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    query = query.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize on spaces\n",
    "    words = query.split(' ')\n",
    "\n",
    "    # Remove stopwords\n",
    "    filtered = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Stem each word in the filtered list\n",
    "    stemmed = [parser.stem(word) for word in filtered]\n",
    "\n",
    "    # Join back to string\n",
    "    processed_query = \" \".join(stemmed)\n",
    "\n",
    "    return processed_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read and return queries from txt file\n",
    "def read_query(filename):\n",
    "    \n",
    "    # Initialize empty dictionary to store queries \n",
    "    queries = {} \n",
    "\n",
    "    # Read file line by line\n",
    "    with open(filename, 'r') as file:\n",
    "        \n",
    "        for line in file:\n",
    "            if not line.strip(): \n",
    "                continue\n",
    "\n",
    "            # Split line to get query number and text\n",
    "            attributes = line.split('.', 1)  \n",
    "\n",
    "            # Extract query number and text\n",
    "            query_number = attributes[0].strip()  \n",
    "            query = attributes[1].strip()\n",
    "\n",
    "            # Use the new query_analyzer for query processing\n",
    "            processed_query = process_query(query)\n",
    "            \n",
    "            # Store processed query text with query number as key\n",
    "            queries[query_number] = processed_query\n",
    "    \n",
    "    # Return dictionary of queries\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Elasticsearch index using match query \n",
    "def es_search(query):\n",
    "    \n",
    "    # Send match query to Elasticsearch\n",
    "    result = es.search(index=index_name, body={'query': {'match': {'text': query}}}, size=1000)\n",
    "\n",
    "    # Return the hits\n",
    "    return result.get('hits', {}).get('hits', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve term vectors\n",
    "def get_vectors(doc_id):\n",
    "    body = {\n",
    "        \"ids\": [doc_id],\n",
    "        \"parameters\": {\n",
    "            \"term_statistics\": True,\n",
    "            \"field_statistics\": True\n",
    "        }\n",
    "    }\n",
    "\n",
    "    result = es.mtermvectors(index = index_name, body = body)\n",
    "                             \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries and variables for document lengths and term frequencies\n",
    "doc_lengths = {}\n",
    "term_frequencies = {}\n",
    "total_length = 0\n",
    "total_docs = 0 \n",
    "\n",
    "for doc in docs:\n",
    "\n",
    "    # Get term vectors for this document\n",
    "    vectors_data = get_vectors(doc['DOCNO'])\n",
    "\n",
    "    # Check if 'term_vectors' has actual text \n",
    "    if 'term_vectors' not in vectors_data['docs'][0] or 'text' not in vectors_data['docs'][0]['term_vectors']:\n",
    "        continue\n",
    "\n",
    "    # Access the term vectors\n",
    "    term_info = vectors_data['docs'][0]['term_vectors']['text']['terms']\n",
    "\n",
    "    # Calculate the document length\n",
    "    doc_length = sum(info['term_freq'] for info in term_info.values())\n",
    "\n",
    "    # Add to total length\n",
    "    total_length += doc_length\n",
    "\n",
    "    # Increment count\n",
    "    total_docs += 1\n",
    "\n",
    "    # Store the document length\n",
    "    doc_lengths[doc['DOCNO']] = doc_length\n",
    "\n",
    "    # Store term frequencies for each term in the document\n",
    "    term_frequencies[doc['DOCNO']] = {term: info['term_freq'] for term, info in term_info.items()}\n",
    "\n",
    " # Calculate average length of documents\n",
    "average_length = total_length / total_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read queries from the specified file\n",
    "queries = read_query(query_path)\n",
    "\n",
    "# Precompute Inverse Document Frequency (IDF) for all terms in the corpus\n",
    "doc_frequencies = {}\n",
    "unique_terms = set() \n",
    "\n",
    "# Iterate through documents and their term frequencies\n",
    "for doc, terms in term_frequencies.items():\n",
    "    \n",
    "    # Iterate through terms in the document\n",
    "    for term in terms:\n",
    "        # Update document frequency for the term\n",
    "        doc_frequencies[term] = doc_frequencies.get(term, 0) + 1\n",
    "        unique_terms.add(term)\n",
    "\n",
    "# Store the number of unique terms in the collection\n",
    "V = len(unique_terms)\n",
    "\n",
    "# Precompute IDF for all terms in the queries\n",
    "query_idfs = {}\n",
    "\n",
    "# Iterate through queries\n",
    "for query_number, query_text in queries.items():\n",
    "    \n",
    "    # Split the query text into individual terms\n",
    "    query_terms = query_text.split()\n",
    "\n",
    "    # Iterate through terms in the query\n",
    "    for term in query_terms:\n",
    "        \n",
    "        # Get the document frequency (df) for the term\n",
    "        df = doc_frequencies.get(term, 0)\n",
    "\n",
    "        # Skip if document frequency is 0 (term not present in the corpus)\n",
    "        if df == 0:\n",
    "            continue\n",
    "\n",
    "        # Calculate Inverse Document Frequency (IDF) for the term\n",
    "        idf = math.log(len(docs) / df)\n",
    "\n",
    "        # Update the query IDF dictionary with the computed IDF for the term\n",
    "   \n",
    "        query_idfs[term] = idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ES Builtin score for a document\n",
    "def es_built_score(doc):\n",
    "  \n",
    "  return doc['_score'] \n",
    "\n",
    "# Calculate Okapi TF score for a term in a document\n",
    "def okapi_tf_score(doc_id):\n",
    "\n",
    "  # Initialize total TF score\n",
    "  tf_score = 0 \n",
    "  \n",
    "  # Loop through each query term\n",
    "  for term in query_terms:\n",
    "\n",
    "    # Get the term frequency for this term\n",
    "    term_freq = term_frequencies[doc_id].get(term, 0)\n",
    "    \n",
    "    # Calculate the TF score for this specific term \n",
    "    tf_term = term_freq / (term_freq + 0.5 + (1.5 * doc_length / average_length))\n",
    "    \n",
    "    # Add the TF score for this term to the total \n",
    "    tf_score += tf_term\n",
    "\n",
    "  # Return the total summed TF score\n",
    "  return tf_score\n",
    "\n",
    "# Calculate TF-IDF score for query terms in a document\n",
    "def tfidf_score(doc_id):\n",
    "\n",
    "    # Get Okapi TF score for just this term\n",
    "    doc_tf = okapi_tf_score(doc_id)\n",
    "    \n",
    "    tfidf_doc_score = 0.0 \n",
    "\n",
    "    # Loop through each query term\n",
    "    for term in query_terms:\n",
    "        \n",
    "        # Get the IDF for the term, using default if not present\n",
    "        idf = query_idfs.get(term, 0) \n",
    "        tfidf_doc_score += doc_tf * idf  \n",
    "\n",
    "    return tfidf_doc_score\n",
    "\n",
    "# BM25 constants\n",
    "k1 = 1.2\n",
    "k2 = 100\n",
    "b = 0.75\n",
    "\n",
    "# Calculate BM25 score for query terms in a document\n",
    "def bm25_score(doc_id):\n",
    "  \n",
    "    bm25_score = 0\n",
    "\n",
    "    # Iterate through query terms\n",
    "    for term in query_terms:\n",
    "\n",
    "        # Get term frequencies\n",
    "        tf_d = term_frequencies[doc_id].get(term, 0)\n",
    "        tf_q = query_terms.count(term)\n",
    "\n",
    "        # Calculate document frequency (df) for the term\n",
    "        df = doc_frequencies.get(term, 0)\n",
    "\n",
    "        # Calculate BM25 formula components\n",
    "        idf_component = math.log((len(docs) + 0.5) / (df + 0.5))\n",
    "        doc_length = doc_lengths[doc_id]\n",
    "        tf_d_component = (tf_d + k1 * tf_d) / (tf_d + k1 * ((1 - b) + b * (doc_length / average_length)))\n",
    "        tf_q_component = (tf_q + k2 * tf_q) / (tf_q + k2)\n",
    "\n",
    "        # Accumulate BM25 score\n",
    "        bm25_score += idf_component * tf_d_component * tf_q_component\n",
    "\n",
    "    return bm25_score\n",
    "\n",
    "# Function to calculate Unigram LM score with Laplace smoothing for a document\n",
    "def lml_score(doc_id):\n",
    "\n",
    "    lm_laplace_score = 0\n",
    "\n",
    "    # Calculate document length\n",
    "    doc_length = doc_lengths[doc_id]\n",
    "\n",
    "    # Iterate through query terms\n",
    "    for term in query_terms:\n",
    "\n",
    "        # Get term frequency  \n",
    "        tf_w_d = term_frequencies[doc_id].get(term, 0) \n",
    "        \n",
    "        # Calculate probability with smoothed tf\n",
    "        p_laplace = (tf_w_d + 1) / (doc_length + V)\n",
    "\n",
    "        # If term is absent, push hard penalty of -1000\n",
    "        if tf_w_d == 0:\n",
    "          lm_laplace_score += -1000\n",
    "          continue\n",
    "\n",
    "        else :\n",
    "          lm_laplace_score += math.log(p_laplace)\n",
    "      \n",
    "    return lm_laplace_score\n",
    "\n",
    "lambda_jm = 0.9 \n",
    "\n",
    "# Function to calculate Unigram LM score with Jelinek-Mercer smoothing parameter\n",
    "def lm_jm_score(doc_id):\n",
    "\n",
    "    lm_jm_score = 0\n",
    "    \n",
    "    # Calculate document length\n",
    "    doc_length = doc_lengths[doc_id]\n",
    "\n",
    "    # Iterate through query terms \n",
    "    for term in query_terms:\n",
    "\n",
    "        # Get term frequency in this document\n",
    "        tf_w_d = term_frequencies[doc_id].get(term, 0)\n",
    "\n",
    "        # Calculate probability with Jelinek-Mercer smoothing using cfw/V\n",
    "        bg_model = term_frequencies.get(term, 0) / total_length\n",
    "        p_jm = lambda_jm * (tf_w_d/doc_length) + (1 - lambda_jm) * bg_model\n",
    "\n",
    "        # Handle the case when p_jm is exactly 0 by adding a penalty score\n",
    "        if p_jm == 0:\n",
    "            lm_jm_score += -1000\n",
    "        else:\n",
    "            # Accumulate log probability\n",
    "            lm_jm_score += math.log(p_jm)\n",
    "\n",
    "    return lm_jm_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining dictionary for scoring functions that will be used for different text files\n",
    "scoring_function = {\n",
    "  'es_built': es_built_score,\n",
    "  'okapi': okapi_tf_score,\n",
    "  'tfidf': tfidf_score, \n",
    "  'BM25': bm25_score,\n",
    "  'LML' : lml_score,\n",
    "  'LMJM' : lm_jm_score\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results(query_number, hits, scoring_model):\n",
    "  \n",
    "  output = []\n",
    "\n",
    "  # Initialize the rank\n",
    "  rank = 1\n",
    "\n",
    "  # Check if there are hits\n",
    "  if hits:\n",
    "\n",
    "    # Define the scoring function based on the model\n",
    "    if scoring_model == \"es_built\":\n",
    "        scoring_function = es_built_score\n",
    "    elif scoring_model == 'okapi':\n",
    "        scoring_function = okapi_tf_score\n",
    "    elif scoring_model == 'tfidf':\n",
    "        scoring_function = tfidf_score\n",
    "    elif scoring_model == 'BM25':\n",
    "        scoring_function = bm25_score\n",
    "    elif scoring_model == 'LML':\n",
    "        scoring_function = lml_score\n",
    "    elif scoring_model == 'LMJM':\n",
    "        scoring_function = lm_jm_score\n",
    "\n",
    "    # Iterate through the hits  \n",
    "    for hit in hits:\n",
    "      \n",
    "      # Get values from hit\n",
    "      docno = hit['_source']['DOCNO']\n",
    "\n",
    "      # Pass the appropriate arguments to the scoring function\n",
    "      if scoring_model == \"es_built\":\n",
    "        score = scoring_function(hit)\n",
    "      elif scoring_model == 'okapi':\n",
    "        score = scoring_function(docno)\n",
    "      elif scoring_model == 'tfidf':\n",
    "        score = scoring_function(docno)\n",
    "      elif scoring_model == 'BM25':\n",
    "        score = scoring_function(docno)\n",
    "      elif scoring_model == 'LML':\n",
    "        score = scoring_function(docno)\n",
    "      elif scoring_model == 'LMJM':\n",
    "        score = scoring_function(docno)\n",
    "\n",
    "      # Create output line \n",
    "      output_line = f\"{query_number} Q0 {docno} {rank} {score} Exp\\n\"\n",
    "      \n",
    "      output.append(output_line)\n",
    "      \n",
    "      # Increment rank\n",
    "      rank += 1\n",
    "  \n",
    "  # Return complete output \n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines directory to add new file\n",
    "output_path = '/Users/reelataher/hw1-Reela-Taher/IR_data/AP_DATA'\n",
    "\n",
    "# Takes output string from search and writes it to a text file in the specified output directory\n",
    "def output_txt(filename, results):\n",
    "    with open(output_path + '/' + filename + '.txt', 'w') as file:\n",
    "        file.write(''.join(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read queries and initialize results variables\n",
    "es_string = ''\n",
    "\n",
    "# Process each query for ES Built-in\n",
    "for query_number, query_text in queries.items():\n",
    "    \n",
    "    # Run search function for text in each query using the new query_search function\n",
    "    hits = es_search(query_text)\n",
    "\n",
    "    # Process results for ES Built-in using es_built_score\n",
    "    es_results = process_results(query_number, hits, \"es_built\")\n",
    "\n",
    "    # Check if there are results before updating string and sort results\n",
    "    if es_results:\n",
    "        es_results.sort(key=lambda x: float(x.split()[-2]), reverse=True)\n",
    "\n",
    "    # Check if there are hits before updating the string\n",
    "    if es_results:\n",
    "        es_string += ''.join(es_results)\n",
    "\n",
    "# Write ES Built-in output file\n",
    "output_txt(\"es_results\", es_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read queries and initialize results variables\n",
    "okapi_string = ''\n",
    "\n",
    "# Process each query for Okapi\n",
    "for query_number, query_text in queries.items():\n",
    "   \n",
    "    # Split query into terms\n",
    "    query_terms = query_text.split()\n",
    "    \n",
    "    # Run search function for text in each query\n",
    "    hits = es_search(query_text)\n",
    "\n",
    "    # Process results for Okapi\n",
    "    okapi_results = process_results(query_number, hits, \"okapi\")\n",
    "\n",
    "    # Check if there are results before updating string and sort results\n",
    "    if okapi_results:\n",
    "        okapi_results.sort(key=lambda x: float(x.split()[-2]), reverse=True)\n",
    "\n",
    "    # Check if there are hits before updating the string\n",
    "    if okapi_results:\n",
    "        okapi_string += ''.join(okapi_results)\n",
    "\n",
    "# Write Okapi output file\n",
    "output_txt(\"okapi_results\", okapi_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read queries and initialize results variables\n",
    "tfidf_string = ''\n",
    "\n",
    "# Process each query for TF-IDF\n",
    "for query_number, query_text in queries.items():\n",
    "   \n",
    "    # Split query into terms\n",
    "    query_terms = query_text.split()\n",
    "\n",
    "    # Run search function for text in each query\n",
    "    hits = es_search(query_text)\n",
    "\n",
    "    # Process results for TF-IDF\n",
    "    tfidf_results = process_results(query_number, hits, \"tfidf\")\n",
    "\n",
    "    # Check if there are results before updating string and sort results\n",
    "    if tfidf_results:\n",
    "        tfidf_results.sort(key=lambda x: float(x.split()[-2]), reverse=True)\n",
    "\n",
    "    # Check if there are hits before updating the string\n",
    "    if tfidf_results:\n",
    "        tfidf_string += ''.join(tfidf_results)\n",
    "\n",
    "# Write TF-IDF output file\n",
    "output_txt(\"tfidf_results\", tfidf_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read queries and initialize results variables\n",
    "bm25_string = ''\n",
    "queries = read_query(query_path)\n",
    "\n",
    "# Process each query for BM25 \n",
    "for query_number, query_text in queries.items():\n",
    "\n",
    "    # Split query into terms\n",
    "    query_terms = query_text.split()  \n",
    "\n",
    "    # Run search function for text in each query\n",
    "    hits = es_search(query_text)\n",
    "\n",
    "    # Process results for BM25\n",
    "    bm25_results = process_results(query_number, hits, \"BM25\")\n",
    "\n",
    "    # Check if there are results before updating string and sort results\n",
    "    if bm25_results:\n",
    "        bm25_results.sort(key=lambda x: float(x.split()[-2]), reverse=True)\n",
    "\n",
    "    # Check if there are hits before updating string\n",
    "    if bm25_results:\n",
    "        bm25_string += ''.join(bm25_results)\n",
    "        \n",
    "# Write BM25 output file\n",
    "output_txt(\"bm25_results\", bm25_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read queries and initialize results variables\n",
    "lm_laplace_string = ''\n",
    " \n",
    "# Process each query for LM Laplace\n",
    "for query_number, query_text in queries.items():\n",
    "\n",
    "    # Split query into terms\n",
    "    query_terms = query_text.split()  \n",
    "\n",
    "    # Run search function for text in each query\n",
    "    hits = es_search(query_text)\n",
    "\n",
    "    # Process results for LM Laplace\n",
    "    lm_laplace_results = process_results(query_number, hits, \"LML\")\n",
    "\n",
    "    # Check if there are results before updating string and sort results\n",
    "    if lm_laplace_results:\n",
    "        lm_laplace_results.sort(key=lambda x: float(x.split()[-2]), reverse=True)\n",
    "\n",
    "    # Check if there are results before updating string\n",
    "    if lm_laplace_results:\n",
    "        lm_laplace_string += ''.join(lm_laplace_results)\n",
    "        \n",
    "# Write LM Laplace output file\n",
    "output_txt(\"lml_results\", lm_laplace_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read queries and initialize results variables\n",
    "lm_jm_string = ''\n",
    "\n",
    "# Process each query for JM\n",
    "for query_number, query_text in queries.items():\n",
    "\n",
    "  # Split query into terms\n",
    "  query_terms = query_text.split()  \n",
    "\n",
    "  # Run search function for text in each query\n",
    "  hits = es_search(query_text)\n",
    "\n",
    "  # Process results for JM\n",
    "  lm_jm_results = process_results(query_number, hits, \"LMJM\")\n",
    "\n",
    "  # Check if there are results before updating string and sort results\n",
    "  if lm_jm_results:\n",
    "    lm_jm_results.sort(key=lambda x: float(x.split()[-2]), reverse=True)\n",
    "\n",
    "    # Check if there are results before updating string\n",
    "    if lm_jm_results:\n",
    "      lm_jm_string += ''.join(lm_jm_results)\n",
    "      \n",
    "# Write JM output file  \n",
    "output_txt(\"lmjm_results\", lm_jm_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
